{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_path=${YOUR_PROJECT_PATH}\n",
    "project_path=\"/apdcephfs/sz/ChartMimic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_passed_files(modelagent):\n",
    "    template_type = modelagent.split(\"_\")[-1].split(\"Agent\")[0].lower()\n",
    "    print(template_type)\n",
    "    file_dir = project_path + \"/results/customized/chartedit_{}_results/{}_checker\".format(modelagent, template_type)\n",
    "    filter_files = os.listdir(file_dir)\n",
    "    filter_files = [ item.split(\".pdf\")[0]+\".py\" for item in filter_files if \".pdf\" in item]\n",
    "\n",
    "    return filter_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\"gpt-4-vision-preview\", \"claude-3-opus-20240229\", \"gemini-pro-vision\", \"Phi-3-vision-128k-instruct\", \"llava-v1.6-vicuna-7b-hf\", \"deepseek-vl-7b-chat\", \"llava-v1.6-mistral-7b-hf\", \"idefics2-8b\", \"MiniCPM-Llama3-V-2_5\", \"Qwen-VL-Chat\", \"llava-v1.6-vicuna-13b-hf\", \"cogvlm2-llama3-chat-19B\", \"InternVL-Chat-V1-5\", \"llava-v1.6-34b-hf\"]\n",
    "\n",
    "models = [\"gpt-4-vision-preview\"]\n",
    "agents = [\"EditAgent\"]\n",
    "\n",
    "table_type = \"all\"\n",
    "\n",
    "model_agents = [ \"{}_{}\".format(model, agent) for model in models for agent in agents ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_type = \"code_pass\"   # or no_filter\n",
    "denominator = 500\n",
    "\n",
    "if filter_type == \"no_filter\":\n",
    "    filter_files_dict = None\n",
    "elif filter_type == \"code_pass\":\n",
    "    filter_files_dict = { model_agent: get_code_passed_files(model_agent) for model_agent in model_agents}\n",
    "else:\n",
    "    raise ValueError(\"filter_type not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dataframe, \"model\" column is the model name\n",
    "result_df = pd.DataFrame(columns=[\"model_agent\", \"example_count\", \"ExecRate\", \"TextScore\",\"LayoutScore\",  \"TypeScore\",  \"ColorScore\", \"Average\", \"GPT4VScore\", \"Overall\"])\n",
    "\n",
    "# insert the model name\n",
    "result_df[\"model_agent\"] = [ model + \"_\" + agent for model in models for agent in agents]\n",
    "# set the index to be the model name\n",
    "result_df.set_index(\"model_agent\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        filename =  project_path + \"/results/customized/chartedit_\" + model + \"_\" + agent +\"_results_code4evaluation.json\"\n",
    "        if os.path.exists(filename):\n",
    "            files.append(filename)\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File not found: {}\".format(filename))\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(\"Processing file:\", os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "    # filter_files = get_code_passed_files(model_agents[idx])\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "        print(\"Length of filter files:\", len(filter_files))\n",
    "\n",
    "    print(\"Length of Data:\", len(data))\n",
    "    print(\"Denominator:\", denominator)\n",
    "\n",
    "    f1s = []\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "    result_df.loc[model_agents[idx], \"ExecRate\"] = len(filter_files) / denominator\n",
    "    print(\"Execution Rate:\", len(filter_files) / denominator)\n",
    "\n",
    "    text_metrics = data[\"text_metrics\"]\n",
    "    avg_f1 = text_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TextScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    layout_metrics = data['layout_metrics']\n",
    "    avg_f1 = layout_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"LayoutScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    chart_type_metrics = data[\"chart_type_metrics\"]\n",
    "    avg_f1 = chart_type_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"TypeScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    color_metrics = data[\"color_metrics\"]\n",
    "    avg_f1 = color_metrics.apply(lambda x: x[\"f1\"]).sum()*100 / denominator\n",
    "    print(avg_f1)\n",
    "    result_df.loc[model_agents[idx], \"ColorScore\"] = avg_f1\n",
    "    f1s.append(avg_f1)\n",
    "\n",
    "    print( sum(f1s)/len(f1s) )\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"Average\"] = sum(f1s)/len(f1s)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for model in models:\n",
    "    for agent in agents:\n",
    "        file =  project_path + \"/results/customized/chartedit_\" + model + \"_\" + agent +\"_results_gpt4v.json\"\n",
    "        if os.path.exists(file):\n",
    "            files.append(file)\n",
    "\n",
    "\n",
    "for idx, file in enumerate(files):\n",
    "    print(os.path.basename(file))\n",
    "    \n",
    "    data = pd.read_json(file, lines=True)\n",
    "    data[\"orginial\"] = data[\"orginial\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "    data[\"generated\"] = data[\"generated\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "    if filter_files_dict is not None:\n",
    "        filter_files = filter_files_dict[model_agents[idx]]\n",
    "        data = data[ data[\"orginial\"].apply(lambda x: any([item == x for item in filter_files])) ]\n",
    "    print(len(data))\n",
    "\n",
    "\n",
    "    result_df.loc[model_agents[idx], \"example_count\"] = len(data)\n",
    "\n",
    "    gpt4v_score = data[\"gpt4v_score\"]\n",
    "    avg_gpt4v_score = gpt4v_score.sum() / denominator\n",
    "    print(model_agents[idx])\n",
    "    result_df.loc[model_agents[idx], \"GPT4VScore\"] = avg_gpt4v_score\n",
    "    print(avg_gpt4v_score)\n",
    "\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the overall score\n",
    "result_df[\"Overall\"] = result_df[[\"Average\", \"GPT4VScore\"]].mean(axis=1)\n",
    "\n",
    "result_df[\"ExecRate\"] = result_df[\"ExecRate\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('results.csv' index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
